{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 whois requests"
      ],
      "metadata": {
        "id": "SpA_zo-GUOG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "URL데이터 파일 업로드 (파일 업로드 따로 할거면 사용 X)"
      ],
      "metadata": {
        "id": "dttxw8zjUMlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "DIcnBg6iUSkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "라이브러리 임포트 및 데이터 로드"
      ],
      "metadata": {
        "id": "-XYDiSIzUV4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import urllib.parse\n",
        "import whois\n",
        "import socket\n",
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# URL 파일 경로\n",
        "url_file_path = 'urls.txt'\n",
        "\n",
        "# URL 데이터를 읽어와 데이터프레임으로 변환\n",
        "urls = pd.read_csv(url_file_path, header=None, names=['url'])"
      ],
      "metadata": {
        "id": "274HIfZUUWan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "피처 수집 함수 정의"
      ],
      "metadata": {
        "id": "lLyvfKoMgzs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import urllib.parse\n",
        "import whois\n",
        "from datetime import datetime, timedelta\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import socket\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# 1. 정규 표현식으로 IP 주소 형식 패턴 정의\n",
        "ip_pattern = r\"(?:\\d{1,3}\\.){3}\\d{1,3}\"\n",
        "\n",
        "# 2. URL 단축 서비스 패턴 정의\n",
        "shortening_services = r\"bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|\" \\\n",
        "                      r\"yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|\" \\\n",
        "                      r\"short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|\" \\\n",
        "                      r\"doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|db\\.tt|\" \\\n",
        "                      r\"qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|q\\.gs|is\\.gd|\" \\\n",
        "                      r\"po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|x\\.co|\" \\\n",
        "                      r\"prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|\" \\\n",
        "                      r\"tr\\.im|link\\.zip\\.net|buff\\.ly|rb\\.gy|rebrand\\.ly|short\\.cm|clk\\.im|cutt\\.ly|t2m\\.io|bl\\.ink|\" \\\n",
        "                      r\"tiny\\.cc\"\n",
        "\n",
        "# 타임아웃 시간 설정\n",
        "TIMEOUT = 3\n",
        "\n",
        "# 피처 수집 함수 정의\n",
        "def extract_features(url):\n",
        "    result = {}\n",
        "\n",
        "    # 1. IP_LIKE\n",
        "    result['IP_LIKE'] = 1 if re.search(ip_pattern, url) else 0\n",
        "\n",
        "    # 2. AT\n",
        "    result['AT'] = 1 if \"@\" in url else 0\n",
        "\n",
        "    # 3. URL_Depth\n",
        "    path = urllib.parse.urlparse(url).path\n",
        "    segments = [segment for segment in path.split('/') if segment]\n",
        "    depth = len(segments)\n",
        "    result['URL_Depth'] = -1 if depth == 0 else (0 if depth == 1 else 1)\n",
        "\n",
        "    # 4. Redirection\n",
        "    result['Redirection'] = 1 if url.rfind('//') > 6 else 0\n",
        "\n",
        "    # 5. Is_Https\n",
        "    result['Is_Https'] = 1 if urllib.parse.urlsplit(url).scheme == 'https' else 0\n",
        "\n",
        "    # 6. TINY_URL\n",
        "    result['TINY_URL'] = 1 if re.search(shortening_services, url) else 0\n",
        "\n",
        "    # 7. Check_Hyphen\n",
        "    result['Check_Hyphen'] = 1 if '-' in url else 0\n",
        "\n",
        "    # 8. Query\n",
        "    def parse_query_string(url):\n",
        "        if '?' not in url:\n",
        "            return 0\n",
        "        query_string = url.split('?')[-1]\n",
        "        return len(query_string.split('&'))\n",
        "\n",
        "    def categorize_query_count(count):\n",
        "        return -1 if count == 0 else (0 if count == 1 else 1)\n",
        "\n",
        "    result['Query'] = categorize_query_count(parse_query_string(url))\n",
        "\n",
        "    # 9. Domain_Age\n",
        "    def is_domain_created(url):\n",
        "        try:\n",
        "            domain_name = urllib.parse.urlsplit(url).netloc\n",
        "            socket.setdefaulttimeout(TIMEOUT)\n",
        "            domain_info = whois.whois(domain_name)\n",
        "            creation_date = domain_info.creation_date\n",
        "\n",
        "            if isinstance(creation_date, list):\n",
        "                creation_date = creation_date[0]\n",
        "\n",
        "            if not isinstance(creation_date, datetime):\n",
        "                return 1\n",
        "\n",
        "            today = datetime.today()\n",
        "            one_years_ago = today - timedelta(days=365)\n",
        "\n",
        "            return 0 if creation_date <= one_years_ago else 1\n",
        "        except Exception:\n",
        "            return 1\n",
        "\n",
        "    result['Domain_Age'] = is_domain_created(url)\n",
        "\n",
        "    # 10. Domain_end\n",
        "    def domain_end(domain_name):\n",
        "        try:\n",
        "            socket.setdefaulttimeout(TIMEOUT)\n",
        "            domain_info = whois.whois(domain_name)\n",
        "            expiration_date = domain_info.expiration_date\n",
        "\n",
        "            if isinstance(expiration_date, list):\n",
        "                expiration_date = expiration_date[0]\n",
        "\n",
        "            if isinstance(expiration_date, str):\n",
        "                expiration_date = datetime.strptime(expiration_date, \"%Y-%m-%d\")\n",
        "\n",
        "            if expiration_date is None:\n",
        "                return 0\n",
        "\n",
        "            if expiration_date.tzinfo is not None:\n",
        "                expiration_date = expiration_date.replace(tzinfo=None)\n",
        "\n",
        "            today = datetime.now()\n",
        "            days_until_expiry = (expiration_date - today).days\n",
        "\n",
        "            return 1 if (days_until_expiry / 30) < 5 else 0\n",
        "        except Exception:\n",
        "            return 1\n",
        "\n",
        "    result['Domain_end'] = domain_end(urllib.parse.urlsplit(url).netloc)\n",
        "\n",
        "    # 11. Mouseover\n",
        "    def check_mouseover(html_content):\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        return 1 if soup.find(attrs={\"onmouseover\": True}) else 0\n",
        "\n",
        "    def check_url(url):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=TIMEOUT)\n",
        "            return check_mouseover(response.text)\n",
        "        except requests.RequestException:\n",
        "            return -1\n",
        "\n",
        "    result['Mouseover'] = check_url(url)\n",
        "\n",
        "    # 12. Web_Forwards\n",
        "    def web_forwards(url):\n",
        "        try:\n",
        "            response = requests.get(url, allow_redirects=True, timeout=TIMEOUT)\n",
        "            return 0 if len(response.history) <= 2 else 1\n",
        "        except requests.RequestException:\n",
        "            return 1\n",
        "\n",
        "    result['Web_forwards'] = web_forwards(url)\n",
        "\n",
        "    # 13. Hyperlinks\n",
        "    def count_hyperlinks(url):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=TIMEOUT)\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            return len(soup.find_all('a'))\n",
        "        except requests.RequestException:\n",
        "            return -1\n",
        "\n",
        "    result['Hyperlinks'] = count_hyperlinks(url)\n",
        "\n",
        "    # 14. Domain Consistency\n",
        "    def check_domain_consistency(url):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=TIMEOUT)\n",
        "            original_domain = urllib.parse.urlparse(url).netloc\n",
        "            final_domain = urllib.parse.urlparse(response.url).netloc\n",
        "            return 1 if original_domain == final_domain else 0\n",
        "        except requests.RequestException:\n",
        "            return -1\n",
        "\n",
        "    result['Domain_Consistency'] = check_domain_consistency(url)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "meIjXYBrgT_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 피처 수집 실행 및 데이터프레임 생성"
      ],
      "metadata": {
        "id": "K1mIqlpdhZpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모든 URL에 대해 피처 수집\n",
        "features = urls['url'].apply(extract_features)\n",
        "\n",
        "# 결과를 데이터프레임으로 변환\n",
        "df_results = pd.DataFrame(features.tolist())\n",
        "\n",
        "# 결과 확인\n",
        "print(df_results.head())"
      ],
      "metadata": {
        "id": "JVQsnurVhqyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터 전처리"
      ],
      "metadata": {
        "id": "NJ8zXcMyVUzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 처리\n",
        "df_results.fillna(0, inplace=True)  # 결측치를 0으로 대체\n",
        "\n",
        "# 데이터 정규화 (Min-Max Scaling 예시)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(df_results.drop(columns=['label']))  # 레이블 제외"
      ],
      "metadata": {
        "id": "yfyF_w1-VWZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "머신러닝 모델 학습"
      ],
      "metadata": {
        "id": "E4Z-dg_lh_DS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# 레이블 데이터 (예시로 0과 1로 구성된 레이블을 생성)\n",
        "# 실제로는 레이블을 별도로 준비해야 함\n",
        "df_results['label'] = [0, 1] * (len(df_results) // 2)  # 예시로 임의의 레이블 추가(수정 필요)\n",
        "\n",
        "# 피처와 레이블 분리\n",
        "X = df_results.drop(columns=['label'])\n",
        "y = df_results['label']\n",
        "\n",
        "# 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 모델 초기화 및 학습\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 예측\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 결과 평가\n",
        "print(classification_report(y_test, y_pred))\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "VvB0bQlqiBgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 성능 평가"
      ],
      "metadata": {
        "id": "w4IcDR8WVYuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# K-Fold 교차 검증\n",
        "scores = cross_val_score(model, X_scaled, y, cv=5)\n",
        "print(\"Cross-validation scores:\", scores)\n",
        "print(\"Mean score:\", scores.mean())"
      ],
      "metadata": {
        "id": "E_rtbOmMVa4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 성능 시각화"
      ],
      "metadata": {
        "id": "iq8ex6hqViov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 혼동 행렬 생성\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# 혼동 행렬 시각화\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Phishing'], yticklabels=['Normal', 'Phishing'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wRhQwPmCVmCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "하이퍼파라미터 튜닝"
      ],
      "metadata": {
        "id": "xkkl7FRqVoRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# 하이퍼파라미터 그리드 설정\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 최적의 하이퍼파라미터 출력\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "best_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "6_q7pabxVn_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델 저장"
      ],
      "metadata": {
        "id": "9xYHctVdiDQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# 모델 저장\n",
        "joblib.dump(model, 'phishing_detection_model.pkl')"
      ],
      "metadata": {
        "id": "MwuJLuYNiFV2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}