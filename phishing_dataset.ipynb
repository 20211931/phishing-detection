{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14582877-c4f3-49f9-9bda-1e7f5edab53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import urllib.parse\n",
    "import whois\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import socket\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd570ca-66eb-44cf-ad4f-6e22f27a22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#1. 정규 표현식으로 IP 주소 형식 패턴 정의(범주X)\n",
    "ip_pattern = r\"(?:\\d{1,3}\\.){3}\\d{1,3}\"\n",
    "\n",
    "# 'IP_LIKE' 열 추가\n",
    "df['IP_LIKE'] = pd.Series(dtype=int)\n",
    "\n",
    "# URL 문자열 내 IP 주소 형식 존재 여부 확인 및 'IP_LIKE' 열 업데이트\n",
    "for idx, url in df['url'].items():\n",
    "    if re.search(ip_pattern, url):\n",
    "        df.loc[idx, 'IP_LIKE'] = 1\n",
    "    else:\n",
    "        df.loc[idx, 'IP_LIKE'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e9b662-725c-4a2b-a83d-59d84d8c7bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.Checks the presence of @ in URL (HaveAt)(범주X)\n",
    "df.loc[:, 'AT'] = pd.Series(dtype=int)\n",
    "\n",
    "def HaveAt(url):\n",
    "    if \"@\" in url:\n",
    "        at = 1\n",
    "    else:\n",
    "        at = 0\n",
    "    return at\n",
    "\n",
    "# Apply the function to each URL and update 'AT' column\n",
    "df.loc[:, 'AT'] = df['url'].apply(HaveAt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d775671-f8fe-4685-b4b9-d3417b3ab11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Gives number of '/' in URL (URL_Depth)(범주X\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "\n",
    "# DataFrame에서 'URL_Depth' 컬럼 생성\n",
    "df.loc[:, 'URL_Depth'] = pd.Series(dtype=int)\n",
    "\n",
    "def getDepth(url):\n",
    "    path = urlparse(url).path\n",
    "    # 슬래시로 분할하여 유효한 부분의 깊이를 계산\n",
    "    segments = [segment for segment in path.split('/') if segment]\n",
    "    depth = len(segments)\n",
    "\n",
    "    # 깊이를 -1, 0, 1로 제한\n",
    "    if depth == 0:\n",
    "        return -1\n",
    "    elif depth == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Apply the function to each URL and update 'URL_Depth' column\n",
    "df.loc[:, 'URL_Depth'] = df['url'].apply(getDepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83b09e13-3fef-4735-9ddd-9916334769ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.Checking for redirection '//' in the url (Redirection)\n",
    "\n",
    "# 새로운 'Redirection' 열을 추가하고 int 형으로 초기화(범주 0)\n",
    "df.loc[:, 'Redirection'] = pd.Series(dtype=int)\n",
    "\n",
    "def redirection(url):\n",
    "    pos = url.rfind('//')  # URL 내에서 마지막으로 나타나는 '//'의 위치를 찾기\n",
    "    if pos > 6:  # 위치가 6보다 크다면\n",
    "        if pos > 7:  # 위치가 7보다 크다면 (즉, 프로토콜 부분을 넘어서 존재하는 경우)\n",
    "            return 1  # 리디렉션이 의심되는 URL로 간주하여 1 반환\n",
    "        else:\n",
    "            return 0  # 그렇지 않다면 0 반환(의심)\n",
    "    else:\n",
    "        return 0  # '//'가 프로토콜 부분에만 있는 경우 0 반환(의심)\n",
    "\n",
    "# 각 URL에 대해 redirection 함수를 적용하고 'Redirection' 열을 업데이트\n",
    "df.loc[:, 'Redirection'] = df['url'].apply(redirection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d71681c2-e74a-47a9-b2df-bea0552efc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.URL의 스킴이 'https'인지(범주X)\n",
    "import urllib.parse\n",
    "\n",
    "def is_https(url):\n",
    "    return 1 if urllib.parse.urlsplit(url).scheme == 'https' else 0\n",
    "\n",
    "df.loc[:, 'Is_Https'] = df['url'].apply(is_https)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c78f6b59-8867-4fc9-9a62-1fc9e0f594e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#6.1 20가지의 주요한 url 단축 서비스 패턴\n",
    "shorteningServices = r\"bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|\" \\\n",
    "                      r\"yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|\" \\\n",
    "                      r\"short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|\" \\\n",
    "                      r\"doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|db\\.tt|\" \\\n",
    "                      r\"qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|q\\.gs|is\\.gd|\" \\\n",
    "                      r\"po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|x\\.co|\" \\\n",
    "                      r\"prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|\" \\\n",
    "                      r\"tr\\.im|link\\.zip\\.net|buff\\.ly|rb\\.gy|rebrand\\.ly|short\\.cm|clk\\.im|cutt\\.ly|t2m\\.io|bl\\.ink|\" \\\n",
    "                      r\"tiny\\.cc\"    #단축서비스 패턴 10개 추가함\n",
    "\n",
    "df.loc[:, 'TINY_URL'] = pd.Series(dtype=int)\n",
    "\n",
    "#6.2 url에 단축서비스가 포함되어있는 지 확인(범주X)\n",
    "def tinyURL(url):\n",
    "    match=re.search(shorteningServices,url) #정규 표현식을 사용하여 입력된 url에서 단축서비스 패턴을 찾음\n",
    "    if match: #입력된 url에 단축서비스 패턴이 있으면 1을 리턴, 아니면 0을 리턴\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the function to each URL and update 'TINY_URL' column\n",
    "df.loc[:, 'TINY_URL'] = df['url'].apply(tinyURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbe6c969-c34d-4f7e-af17-c8e632eff815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. -이 있는지 (범주X)\n",
    "def check_Hyphen(domain):\n",
    "    if '-' in domain:\n",
    "        return 1 \n",
    "    else:\n",
    "        return 0 \n",
    "\n",
    "df['Check_Hyphen'] = df['url'].apply(check_Hyphen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45d4cd1a-5aa5-47a9-bd03-4c5657e58d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.쿼리문자열의 개수 (범주X)\n",
    "def parse_query_string(url):\n",
    "    # URL에서 '?' 문자열이 있는지 확인하여 쿼리 문자열이 있는지 판별\n",
    "    if '?' not in url:\n",
    "        return 0\n",
    "\n",
    "    # URL에서 쿼리 문자열 추출\n",
    "    query_string = url.split('?')[-1]\n",
    "\n",
    "    # 쿼리 문자열을 '&'로 분할하여 각 쌍을 추출\n",
    "    query_pairs = query_string.split('&')\n",
    "\n",
    "    # 각 쌍을 이름과 값으로 분할하여 딕셔너리에 저장\n",
    "    params = {}\n",
    "    for pair in query_pairs:\n",
    "        if '=' in pair:\n",
    "            key, value = pair.split('=', 1)\n",
    "            params[key] = value\n",
    "        else:\n",
    "            params[pair] = None\n",
    "\n",
    "    return len(params)\n",
    "\n",
    "def categorize_query_count(count):\n",
    "    if count == 0:\n",
    "        return -1\n",
    "    elif count == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def Query(url):\n",
    "    query_count = parse_query_string(url)\n",
    "    return categorize_query_count(query_count)\n",
    "\n",
    "# 쿼리 문자열의 개수를 업데이트하여 범주화\n",
    "df['Query'] = df['url'].apply(Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1957430-86ca-42ee-9021-8f63f6cd2b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.도메인 생성일 기준 (범주)\n",
    "\n",
    "# 타임아웃 시간(초) 설정\n",
    "TIMEOUT = 3\n",
    "\n",
    "def is_domain_created(url):\n",
    "    try:\n",
    "        domain_name = urllib.parse.urlsplit(url).netloc\n",
    "        socket.setdefaulttimeout(TIMEOUT)  # 타임아웃 설정\n",
    "        domain_info = whois.whois(domain_name)\n",
    "        creation_date = domain_info.creation_date\n",
    "\n",
    "        # creation_date가 list인 경우 첫 번째 요소를 사용\n",
    "        if isinstance(creation_date, list):\n",
    "            creation_date = creation_date[0]\n",
    "\n",
    "        # creation_date가 datetime 형식이 아닌 경우 처리\n",
    "        if not isinstance(creation_date, datetime):\n",
    "            return 1  # 피싱 사이트로 간주\n",
    "\n",
    "        today = datetime.today()\n",
    "        one_years_ago = today - timedelta(days=365)\n",
    "\n",
    "        if creation_date <= one_years_ago:\n",
    "            return 0  # 1년 이상된 경우 피싱 사이트로 간주하지 않음\n",
    "        else:\n",
    "            return 1  # 1년 이하(피싱 사이트 의심)\n",
    " \n",
    "    except (whois.parser.PywhoisError, socket.timeout, Exception) as e:\n",
    "        return 1  # 오류 발생 시 1(피싱 사이트라고 간주)\n",
    "\n",
    "\n",
    "# 'Domain_Age' 열 추가\n",
    "df['Domain_Age'] = df['url'].apply(is_domain_created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eecb63f-625d-4519-8aaa-1a8d139f9846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.도메인 만료일 기준 (범주)\n",
    "\n",
    "# 타임아웃 시간(초) 설정\n",
    "TIMEOUT = 3\n",
    "\n",
    "# 링크에서 도메인을 추출하는 함수\n",
    "def get_domain_from_link(link):\n",
    "    parsed_uri = urlparse(link)\n",
    "    domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "    return domain\n",
    "\n",
    "# 도메인 만료 여부를 확인하는 함수\n",
    "def domain_end(domain_name):\n",
    "    try:\n",
    "        socket.setdefaulttimeout(TIMEOUT)\n",
    "        domain_info = whois.whois(domain_name)\n",
    "        expiration_date = domain_info.expiration_date\n",
    "\n",
    "        if isinstance(expiration_date, list):\n",
    "            expiration_date = expiration_date[0]\n",
    "\n",
    "        if isinstance(expiration_date, str):\n",
    "            expiration_date = datetime.strptime(expiration_date, \"%Y-%m-%d\")\n",
    "\n",
    "        if expiration_date is None:\n",
    "            return 0\n",
    "\n",
    "        if expiration_date.tzinfo is not None:\n",
    "            expiration_date = expiration_date.replace(tzinfo=None)\n",
    "\n",
    "        today = datetime.now()\n",
    "        days_until_expiry = (expiration_date - today).days\n",
    "\n",
    "        if (days_until_expiry / 30) < 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except (whois.parser.PywhoisError, socket.timeout, Exception):\n",
    "        return 1\n",
    "\n",
    "# 각 URL에 대해 도메인 만료 여부를 확인하는 함수\n",
    "def is_domain_created_within_six_months(url):\n",
    "    domain_name = get_domain_from_link(url)\n",
    "    return domain_end(domain_name)\n",
    "\n",
    "# 병렬 처리 함수\n",
    "def apply_parallel(df, func):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(func, df['url']))\n",
    "    return results\n",
    "\n",
    "# 병렬로 'Domain_end' 컬럼 생성 및 함수 적용\n",
    "df['Domain_end'] = apply_parallel(df, is_domain_created_within_six_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "802a3e2f-100d-4a9b-a4ef-b5d6f0eea6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. Mousover\n",
    "\n",
    "def check_mouseover(html_content):\n",
    "    # BeautifulSoup을 사용하여 HTML 파싱\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # onmouseover 속성을 가진 태그를 찾음\n",
    "    if soup.find(attrs={\"onmouseover\": True}):\n",
    "        return 1  # 마우스 오버 발생\n",
    "    else:\n",
    "        return 0  # 마우스 오버 발생 X\n",
    "\n",
    "def check_url(url):\n",
    "    try:\n",
    "        # URL 파싱을 시도하여 유효성 검사\n",
    "        parsed_url = urlparse(url)\n",
    "        if not all([parsed_url.scheme, parsed_url.netloc]):\n",
    "            raise ValueError(\"Invalid URL\")\n",
    "\n",
    "        response = requests.get(url, timeout=3)  # 타임아웃을 3초로 설정\n",
    "        response.raise_for_status()  # HTTP 에러가 발생하면 예외 발생\n",
    "        response_text = response.text\n",
    "        result = check_mouseover(response_text)\n",
    "    except (requests.exceptions.RequestException, ValueError) as e:\n",
    "        result = -1  # 타임아웃 또는 다른 요청 예외 발생 시 -1을 반환\n",
    "    return result\n",
    "\n",
    "# 'url' 컬럼의 각 URL에 대해 Mouseover 여부를 확인하여 'Mouseover' 컬럼에 결과를 저장\n",
    "df['Mouseover'] = df['url'].apply(check_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b73bdb71-55d4-4970-9c61-3df559026f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12. Web_Forwards(범주)\n",
    "import requests\n",
    "\n",
    "# 웹 포워딩 체크 함수\n",
    "def web_forwards(url, timeout=3):\n",
    "    try:\n",
    "        respon = requests.get(url, allow_redirects=True, timeout=timeout)\n",
    "    except requests.RequestException:\n",
    "        return 1  # 요청 중 오류가 발생하면 피싱 사이트로 간주\n",
    "\n",
    "    if not respon.history:\n",
    "        return 0  # 리디렉션이 전혀 없는 경우 의심 사이트\n",
    "\n",
    "    # 리디렉션 횟수가 2회 이하인 경우 의심 사이트로 간주\n",
    "    if len(respon.history) <= 2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1  # 리디렉션이 3회 이상인 경우 피싱 사이트로 간주\n",
    "\n",
    "# 병렬 처리를 사용하여 각 URL에 대해 web_forwards 함수 실행\n",
    "def parallel_apply(urls, func, max_workers=5):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_url = {executor.submit(func, url): url for url in urls}\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "            except Exception as exc:\n",
    "                result = 1  # 예외 발생 시 피싱 사이트로 간주\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "# 'url' 컬럼의 각 URL에 대해 리디렉션 여부를 확인하여 'Web_forwards' 컬럼에 결과를 저장\n",
    "df['Web_forwards'] = parallel_apply(df['url'], web_forwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea28ed-3115-4a1e-ad1e-48d3d04bcfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13. URL이 실제로 갖고 있는 <a> 태그의 수 (범주X)\n",
    "\n",
    "# 타임아웃 시간(초) 설정\n",
    "TIMEOUT = 3\n",
    "\n",
    "def count_hyperlinks(url):\n",
    "    global a\n",
    "    try:\n",
    "        response = requests.get(url, timeout=TIMEOUT)\n",
    "        response.raise_for_status()  # 요청이 성공했는지 확인\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        hyperlinks = soup.find_all('a')\n",
    "        \n",
    "        return len(hyperlinks)\n",
    "        \n",
    "    except (requests.RequestException, requests.Timeout):\n",
    "        return -1  # 요청 실패 시 -1 반환\n",
    "\n",
    "\n",
    "# 하이퍼링크의 총 개수를 업데이트\n",
    "df['Hyperlinks'] = df['url'].apply(count_hyperlinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bbadf6a-f86f-4573-8faf-5e40c983b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14. 도메인 일관성을 확인하는 함수(범주)\n",
    "\n",
    "# 도메인 일관성 체크 함수\n",
    "def check_domain_consistency(url, timeout=3):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=timeout)\n",
    "        original_domain = urlparse(url).netloc\n",
    "        final_domain = urlparse(response.url).netloc\n",
    "        is_same_domain = 1 if original_domain == final_domain else 0\n",
    "        return is_same_domain\n",
    "    except requests.RequestException:\n",
    "        return -1\n",
    "\n",
    "# 병렬 처리를 사용하여 각 URL에 대해 check_domain_consistency 함수 실행\n",
    "def parallel_apply(urls, func, max_workers=5):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_url = {executor.submit(func, url): url for url in urls}\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "            except Exception as exc:\n",
    "                result = -1  # 예외 발생 시 -1 반환\n",
    "            results.append(result)\n",
    "    return results\n",
    "\n",
    "# 'url' 컬럼의 각 URL에 대해 도메인 일관성을 확인하여 'Domain_Cons' 컬럼에 결과를 저장\n",
    "df['Domain_Cons'] = parallel_apply(df['url'], check_domain_consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0235b66e-8410-42ef-9ef3-5ba90e0951c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def tokenize_url(url: str):\n",
    "    \"\"\"\n",
    "    주어진 URL을 토큰화하여 리스트로 반환하는 함수.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): 토큰화할 URL 문자열.\n",
    "\n",
    "    Returns:\n",
    "    list: URL의 토큰 리스트.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    # URL 파싱\n",
    "    parsed_url = urlparse(url)\n",
    "    \n",
    "    # 도메인 토큰화\n",
    "    domain_tokens = parsed_url.netloc.split('.')\n",
    "    tokens.extend(domain_tokens)\n",
    "    \n",
    "    # 경로 토큰화\n",
    "    path_tokens = parsed_url.path.split('/')\n",
    "    tokens.extend([token for token in path_tokens if token])\n",
    "    \n",
    "    # 쿼리 매개변수 토큰화\n",
    "    query_tokens = parse_qs(parsed_url.query)\n",
    "    for key, values in query_tokens.items():\n",
    "        tokens.append(key)\n",
    "        tokens.extend(values)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "df['Tokenized_url'] = df['url'].apply(lambda x: ' '.join(tokenize_url(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0d9ebf8-d113-47f1-846b-36fe0ff46827",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f70ae1ff-0521-43ea-9b60-001c99b0412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('valid_phish_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcd60d7e-275f-42ed-9269-95f79ef73462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>IP_LIKE</th>\n",
       "      <th>AT</th>\n",
       "      <th>URL_Depth</th>\n",
       "      <th>Redirection</th>\n",
       "      <th>Is_Https</th>\n",
       "      <th>TINY_URL</th>\n",
       "      <th>Check_Hyphen</th>\n",
       "      <th>Query</th>\n",
       "      <th>Domain_Age</th>\n",
       "      <th>Domain_end</th>\n",
       "      <th>Mouseover</th>\n",
       "      <th>Web_forwards</th>\n",
       "      <th>Hyperlinks</th>\n",
       "      <th>Domain_Cons</th>\n",
       "      <th>Tokenized_url</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://allegrolokalnie.oferta-65492.pl/oferta...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>allegrolokalnie oferta-65492 pl oferta l Jeden...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://pub-d0d0d5916dbd4da5b9fe7ed070e51dab.r...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>pub-d0d0d5916dbd4da5b9fe7ed070e51dab r2 dev ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://anmeldung.client-eportal.is-certified....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>anmeldung client-eportal is-certified com acce...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://malerbetrieb-steinin.blogdns.org/.kunde...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>malerbetrieb-steinin blogdns org .kundenarea</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://aktualisieren.83-229-87-162.cprapid.co...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>aktualisieren 83-229-87-162 cprapid com aktf43...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>http://www.gwenet.org/</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>www gwenet org</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>http://files.planetaryedges.workers.dev</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>files planetaryedges workers dev</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>https://docs.google.com/presentation/d/e/2PACX...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>docs google com presentation d e 2PACX-1vRgVRg...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>https://docs.google.com/presentation/d/e/2PACX...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>docs google com presentation d e 2PACX-1vTEHkE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>https://docs.google.com/presentation/d/e/2PACX...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>docs google com presentation d e 2PACX-1vQpun6...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url  IP_LIKE   AT  \\\n",
       "0      https://allegrolokalnie.oferta-65492.pl/oferta...      0.0  0.0   \n",
       "1      https://pub-d0d0d5916dbd4da5b9fe7ed070e51dab.r...      0.0  0.0   \n",
       "2      https://anmeldung.client-eportal.is-certified....      0.0  0.0   \n",
       "3      http://malerbetrieb-steinin.blogdns.org/.kunde...      0.0  0.0   \n",
       "4      https://aktualisieren.83-229-87-162.cprapid.co...      0.0  0.0   \n",
       "...                                                  ...      ...  ...   \n",
       "19995                             http://www.gwenet.org/      0.0  0.0   \n",
       "19996            http://files.planetaryedges.workers.dev      0.0  0.0   \n",
       "19997  https://docs.google.com/presentation/d/e/2PACX...      0.0  0.0   \n",
       "19998  https://docs.google.com/presentation/d/e/2PACX...      0.0  0.0   \n",
       "19999  https://docs.google.com/presentation/d/e/2PACX...      0.0  0.0   \n",
       "\n",
       "       URL_Depth  Redirection  Is_Https  TINY_URL  Check_Hyphen  Query  \\\n",
       "0            1.0          0.0         1       0.0             1      0   \n",
       "1            0.0          0.0         1       0.0             1     -1   \n",
       "2            1.0          0.0         1       0.0             1     -1   \n",
       "3            0.0          0.0         0       0.0             1     -1   \n",
       "4            1.0          0.0         1       0.0             1     -1   \n",
       "...          ...          ...       ...       ...           ...    ...   \n",
       "19995       -1.0          0.0         0       0.0             0     -1   \n",
       "19996       -1.0          0.0         0       0.0             0     -1   \n",
       "19997        1.0          0.0         1       0.0             1      1   \n",
       "19998        1.0          0.0         1       0.0             1      1   \n",
       "19999        1.0          0.0         1       0.0             1      1   \n",
       "\n",
       "       Domain_Age  Domain_end  Mouseover  Web_forwards  Hyperlinks  \\\n",
       "0               1           0         -1             1          -1   \n",
       "1               0           0          0             1          -1   \n",
       "2               1           1         -1             1          -1   \n",
       "3               1           0         -1             1          -1   \n",
       "4               0           0          0             1          -1   \n",
       "...           ...         ...        ...           ...         ...   \n",
       "19995           0           1         -1             1          -1   \n",
       "19996           1           0         -1             0          -1   \n",
       "19997           0           0          0             0           0   \n",
       "19998           0           0          0             0           0   \n",
       "19999           0           0          0             0           0   \n",
       "\n",
       "       Domain_Cons                                      Tokenized_url  Label  \n",
       "0               -1  allegrolokalnie oferta-65492 pl oferta l Jeden...      1  \n",
       "1               -1  pub-d0d0d5916dbd4da5b9fe7ed070e51dab r2 dev ar...      1  \n",
       "2               -1  anmeldung client-eportal is-certified com acce...      1  \n",
       "3               -1       malerbetrieb-steinin blogdns org .kundenarea      1  \n",
       "4               -1  aktualisieren 83-229-87-162 cprapid com aktf43...      1  \n",
       "...            ...                                                ...    ...  \n",
       "19995            1                                     www gwenet org      1  \n",
       "19996            1                   files planetaryedges workers dev      1  \n",
       "19997            1  docs google com presentation d e 2PACX-1vRgVRg...      1  \n",
       "19998            1  docs google com presentation d e 2PACX-1vTEHkE...      1  \n",
       "19999            1  docs google com presentation d e 2PACX-1vQpun6...      1  \n",
       "\n",
       "[20000 rows x 17 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8307f94f-6efe-40c9-9fd0-db3eecab7ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
